{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b37ceb7-b428-4911-983e-176d802fe76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (Embedding, Dense, Dropout, Input, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D)\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b512d7c3-2dd2-4d7d-8693-34e0735d0b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset\n",
    "# metadata = pd.read_csv('../input/flickr-image-dataset/flickr30k_images/results.csv', delimiter='|', engine='python')\n",
    "# metadata = metadata.dropna()\n",
    "\n",
    "# # Image loading function\n",
    "# def load_image(name):\n",
    "#     img = image.load_img(name, target_size=(32, 32, 3))\n",
    "#     img = image.img_to_array(img)\n",
    "#     img = np.reshape(img, (32 * 32 * 3))\n",
    "#     return img\n",
    "\n",
    "# # Prepare image and sentence arrays\n",
    "# image_arr = []\n",
    "# sentence_arr = []\n",
    "# for ind in range(5000):\n",
    "#     if ind % 5 != 0:\n",
    "#         continue\n",
    "#     image_location = metadata.iloc[ind]['image_name']\n",
    "#     sentence = metadata.iloc[ind][' comment']\n",
    "#     image_arr.append(load_image('../input/flickr-image-dataset/flickr30k_images/flickr30k_images/' + str(image_location)))\n",
    "#     sentence_arr.append('<SOS>' + sentence + '<EOS>')\n",
    "\n",
    "# Images = np.array(image_arr)\n",
    "\n",
    "# # Tokenization and padding\n",
    "# def tokenize(x):\n",
    "#     tokenizer = Tokenizer()\n",
    "#     tokenizer.fit_on_texts(x)\n",
    "#     t = tokenizer.texts_to_sequences(x)\n",
    "#     return t, tokenizer\n",
    "\n",
    "# def pad(x, length=None):\n",
    "#     return pad_sequences(x, padding='post', maxlen=length)\n",
    "\n",
    "# def preprocess(sentences):\n",
    "#     text_tokenized, text_tokenizer = tokenize(sentences)\n",
    "#     text_pad = pad(text_tokenized)\n",
    "#     return text_pad, text_tokenizer\n",
    "\n",
    "# Sentence, token_Sentence = preprocess(sentence_arr)\n",
    "\n",
    "# # Transformer model class\n",
    "# class TransformerModel(tf.keras.Model):\n",
    "#     def __init__(self, src_vocab_size, trg_vocab_size, embedding_size, num_heads,\n",
    "#                  num_encoder_layers, num_decoder_layers, forward_expansion, dropout,\n",
    "#                  max_len_s, max_len_t):\n",
    "#         super(TransformerModel, self).__init__()\n",
    "\n",
    "#         self.src_embedding = Embedding(src_vocab_size, embedding_size)\n",
    "#         self.trg_embedding = Embedding(trg_vocab_size, embedding_size)\n",
    "#         self.src_pos_embedding = Embedding(max_len_s, embedding_size)\n",
    "#         self.trg_pos_embedding = Embedding(max_len_t, embedding_size)\n",
    "\n",
    "#         self.encoder_layers = [\n",
    "#             tf.keras.Sequential([\n",
    "#                 MultiHeadAttention(num_heads, key_dim=embedding_size),\n",
    "#                 LayerNormalization(),\n",
    "#                 Dense(forward_expansion * embedding_size, activation='relu'),\n",
    "#                 Dropout(dropout),\n",
    "#                 Dense(embedding_size),\n",
    "#                 LayerNormalization()\n",
    "#             ]) for _ in range(num_encoder_layers)\n",
    "#         ]\n",
    "\n",
    "#         self.decoder_layers = [\n",
    "#             tf.keras.Sequential([\n",
    "#                 MultiHeadAttention(num_heads, key_dim=embedding_size),\n",
    "#                 LayerNormalization(),\n",
    "#                 Dense(forward_expansion * embedding_size, activation='relu'),\n",
    "#                 Dropout(dropout),\n",
    "#                 Dense(embedding_size),\n",
    "#                 LayerNormalization()\n",
    "#             ]) for _ in range(num_decoder_layers)\n",
    "#         ]\n",
    "\n",
    "#         self.final_layer = Dense(trg_vocab_size)\n",
    "\n",
    "#     def call(self, src, trg):\n",
    "#         seq_len_src = tf.shape(src)[1]\n",
    "#         seq_len_trg = tf.shape(trg)[1]\n",
    "\n",
    "#         src_positions = tf.range(start=0, limit=seq_len_src, delta=1)\n",
    "#         trg_positions = tf.range(start=0, limit=seq_len_trg, delta=1)\n",
    "\n",
    "#         src_embeddings = self.src_embedding(src) + self.src_pos_embedding(src_positions)\n",
    "#         trg_embeddings = self.trg_embedding(trg) + self.trg_pos_embedding(trg_positions)\n",
    "\n",
    "#         for layer in self.encoder_layers:\n",
    "#             src_embeddings = layer(src_embeddings)\n",
    "\n",
    "#         for layer in self.decoder_layers:\n",
    "#             trg_embeddings = layer(trg_embeddings)\n",
    "\n",
    "#         outputs = self.final_layer(trg_embeddings)\n",
    "#         return outputs\n",
    "\n",
    "# # Hyperparameters\n",
    "# src_vocab_size = 256\n",
    "# trg_vocab_size = len(token_Sentence.word_index)\n",
    "# embedding_size = 512\n",
    "# num_heads = 8\n",
    "# num_encoder_layers = 3\n",
    "# num_decoder_layers = 3\n",
    "# dropout = 0.10\n",
    "# max_len_s = Images.shape[1]\n",
    "# max_len_t = len(Sentence[0])\n",
    "# forward_expansion = 4\n",
    "# learning_rate = 3e-4\n",
    "# batch_size = 1\n",
    "\n",
    "# model = TransformerModel(\n",
    "#     src_vocab_size, trg_vocab_size, embedding_size, num_heads,\n",
    "#     num_encoder_layers, num_decoder_layers, forward_expansion, dropout,\n",
    "#     max_len_s, max_len_t\n",
    "# )\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# # Training function\n",
    "# def create_batch(src, tar, batchsize, i):\n",
    "#     src, tar = src[(i-1)*batchsize:(i-1)*batchsize + batchsize], tar[(i-1)*batchsize:(i-1)*batchsize + batchsize]\n",
    "#     return tf.convert_to_tensor(src, dtype=tf.int32), tf.convert_to_tensor(tar, dtype=tf.int32)\n",
    "\n",
    "# def train_step(src, tar):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         predictions = model(src, tar[:, :-1])\n",
    "#         loss = loss_object(tar[:, 1:], predictions)\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "#     return loss\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(1, 3):\n",
    "#     total_loss = 0\n",
    "#     for i in range(1, len(Images) // batch_size):\n",
    "#         src, tar = create_batch(Images, Sentence, batch_size, i)\n",
    "#         loss = train_step(src, tar)\n",
    "#         total_loss += loss\n",
    "\n",
    "#     print(f'| end of epoch {epoch} | Training loss {total_loss.numpy():.2f} |')\n",
    "\n",
    "# # Evaluation and image display\n",
    "# def display_image(name):\n",
    "#     img = image.load_img(name, target_size=(512, 512, 3))\n",
    "#     img = image.img_to_array(img)\n",
    "#     img = img / 255\n",
    "#     plt.imshow(img)\n",
    "\n",
    "# def evaluate(index):\n",
    "#     image_location, sent = metadata.iloc[index]['image_name'], metadata.iloc[index][' comment']\n",
    "#     img = load_image('../input/flickr-image-dataset/flickr30k_images/flickr30k_images/' + str(image_location))\n",
    "#     img_arr = np.array([img])\n",
    "#     sentence = [f'<SOS> {sent} <EOS>']\n",
    "#     sentence = pad(token_Sentence.texts_to_sequences(sentence), length=max_len_t)\n",
    "\n",
    "#     src = tf.convert_to_tensor(img_arr, dtype=tf.int32)\n",
    "#     tar = tf.convert_to_tensor(sentence, dtype=tf.int32)\n",
    "\n",
    "#     predictions = model(src, tar[:, :-1])\n",
    "#     sentence_formed = ''\n",
    "#     predicted_ids = tf.argmax(predictions, axis=-1).numpy()[0]\n",
    "#     for word_id in predicted_ids:\n",
    "#         if word_id == 3:  # EOS\n",
    "#             break\n",
    "#         for key, value in token_Sentence.word_index.items():\n",
    "#             if value == word_id and value != 2:  # SOS\n",
    "#                 sentence_formed += key + ' '\n",
    "#                 break\n",
    "\n",
    "#     display_image('../input/flickr-image-dataset/flickr30k_images/flickr30k_images/' + str(image_location))\n",
    "#     return sentence_formed\n",
    "\n",
    "# # Example evaluation\n",
    "# evaluate(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30dc2fc8-df81-4743-9120-8e1cd5008303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import (Embedding, Dense, Dropout, Input, LayerNormalization, MultiHeadAttention, GlobalAveragePooling1D)\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6588af0-90e6-4c1a-b5de-908cf1f6f74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000203564.jpg</td>\n",
       "      <td>A bicycle replica with a clock as the front wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000000322141.jpg</td>\n",
       "      <td>A room with blue walls and a white sink and door.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000000016977.jpg</td>\n",
       "      <td>A car that seems to be parked illegally behind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000000106140.jpg</td>\n",
       "      <td>A large passenger airplane flying through the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000000106140.jpg</td>\n",
       "      <td>There is a GOL plane taking off in a partly cl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              image                                            caption\n",
       "0  000000203564.jpg  A bicycle replica with a clock as the front wh...\n",
       "1  000000322141.jpg  A room with blue walls and a white sink and door.\n",
       "2  000000016977.jpg  A car that seems to be parked illegally behind...\n",
       "3  000000106140.jpg  A large passenger airplane flying through the ...\n",
       "4  000000106140.jpg  There is a GOL plane taking off in a partly cl..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "base_dir = r'C:\\Users\\UTKARSH\\Desktop\\visual intelligence'\n",
    "data_dir = r'C:\\Users\\UTKARSH\\Desktop\\visual intelligence\\data'\n",
    "models_dir = r'C:\\Users\\UTKARSH\\Desktop\\visual intelligence\\models'\n",
    "pre_processed_data_dir = r'C:\\Users\\UTKARSH\\Desktop\\visual intelligence\\pre_processed_data'\n",
    "\n",
    "\n",
    "# File path to the annotations JSON file\n",
    "train_annotations_file = os.path.join(data_dir,'train_captions.json')\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(train_annotations_file):\n",
    "    raise FileNotFoundError(f\"File not found: {train_annotations_file}\")\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(train_annotations_file,'r') as f:\n",
    "    data = json.load(f)\n",
    "    data = data['annotations']\n",
    "\n",
    "# Prepare image-caption pairs\n",
    "img_cap_pairs = []\n",
    "\n",
    "for sample in data:\n",
    "    img_name = '%012d.jpg' % sample['image_id']  # Format image name\n",
    "    img_cap_pairs.append([img_name, sample['caption']])\n",
    "\n",
    "# Create train_captions.txt\n",
    "with open(os.path.join(pre_processed_data_dir, 'train_captions.txt'), 'w') as file:\n",
    "    for item in img_cap_pairs:\n",
    "        file.write(f\"{item[0]},{item[1]}\\n\")\n",
    "\n",
    "# Create a DataFrame with the image-caption pairs\n",
    "train_captions = pd.DataFrame(img_cap_pairs, columns=['image', 'caption'])\n",
    "train_captions.to_csv(os.path.join(pre_processed_data_dir, 'train_captions.csv'), index=False)\n",
    "train_captions = pd.read_csv(os.path.join(pre_processed_data_dir, 'train_captions.csv'))\n",
    "train_captions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd37644d-d943-4acc-ab7d-1d8a92da49a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              image                                            caption\n",
      "0  000000179765.jpg  A black Honda motorcycle parked in front of a ...\n",
      "1  000000179765.jpg      A Honda motorcycle parked in a grass driveway\n",
      "2  000000190236.jpg  An office cubicle with four different types of...\n",
      "3  000000331352.jpg          A small closed toilet in a cramped space.\n",
      "4  000000517069.jpg     Two women waiting at a bench next to a street.\n"
     ]
    }
   ],
   "source": [
    "# File path to the annotations JSON file\n",
    "test_annotations_file = os.path.join(data_dir,'test_captions.json')\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(test_annotations_file):\n",
    "    raise FileNotFoundError(f\"File not found: {test_annotations_file}\")\n",
    "\n",
    "# Open and load the JSON file\n",
    "with open(test_annotations_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    data = data['annotations']\n",
    "\n",
    "# Prepare image-caption pairs\n",
    "img_cap_pairs = []\n",
    "\n",
    "for sample in data:\n",
    "    img_name = '%012d.jpg' % sample['image_id']  # Format image name\n",
    "    img_cap_pairs.append([img_name, sample['caption']])\n",
    "\n",
    "# Create val_captions.txt\n",
    "with open(os.path.join(pre_processed_data_dir, 'test_captions.txt'), 'w') as file:\n",
    "    for item in img_cap_pairs:\n",
    "        file.write(f\"{item[0]},{item[1]}\\n\")\n",
    "\n",
    "# Create a DataFrame with the image-caption pairs\n",
    "test_captions = pd.DataFrame(img_cap_pairs, columns=['image', 'caption'])\n",
    "test_captions.to_csv(os.path.join(pre_processed_data_dir, 'test_captions.csv'), index=False)\n",
    "test_captions = pd.read_csv(os.path.join(pre_processed_data_dir, 'test_captions.csv'))\n",
    "print(test_captions.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96aa060f-112b-4cf6-b1e0-b7bb54d984d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Image Loading Function\n",
    "# def load_image(name):\n",
    "#     img_path = os.path.join(data_dir, 'images', name)  # Adjust path if needed\n",
    "#     img = image.load_img(img_path, target_size=(32, 32, 3))\n",
    "#     img = image.img_to_array(img)\n",
    "#     img = np.reshape(img, (32 * 32 * 3))\n",
    "#     return img\n",
    "\n",
    "# # Process Images and Captions\n",
    "# image_arr, sentence_arr = [], []\n",
    "# for index, row in train_captions.iterrows():\n",
    "#     img_path = row['image']\n",
    "#     caption = '<SOS> ' + row['caption'] + ' <EOS>'\n",
    "#     image_arr.append(load_image(img_path))\n",
    "#     sentence_arr.append(caption)\n",
    "\n",
    "# Images = np.array(image_arr)\n",
    "\n",
    "# # Tokenization and Padding\n",
    "# def tokenize(texts):\n",
    "#     tokenizer = Tokenizer()\n",
    "#     tokenizer.fit_on_texts(texts)\n",
    "#     sequences = tokenizer.texts_to_sequences(texts)\n",
    "#     return sequences, tokenizer\n",
    "\n",
    "# def pad_sequences_fixed(sequences, maxlen=None):\n",
    "#     return pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "\n",
    "# Sentence, token_Sentence = tokenize(sentence_arr)\n",
    "# Sentence = pad_sequences_fixed(Sentence)\n",
    "\n",
    "# # Transformer Model Class\n",
    "# class TransformerModel(tf.keras.Model):\n",
    "#     def __init__(self, src_vocab_size, trg_vocab_size, embedding_size, num_heads,\n",
    "#                  num_encoder_layers, num_decoder_layers, forward_expansion, dropout,\n",
    "#                  max_len_s, max_len_t):\n",
    "#         super(TransformerModel, self).__init__()\n",
    "\n",
    "#         self.src_embedding = Embedding(src_vocab_size, embedding_size)\n",
    "#         self.trg_embedding = Embedding(trg_vocab_size, embedding_size)\n",
    "#         self.src_pos_embedding = Embedding(max_len_s, embedding_size)\n",
    "#         self.trg_pos_embedding = Embedding(max_len_t, embedding_size)\n",
    "\n",
    "#         self.encoder_layers = [\n",
    "#             tf.keras.Sequential([\n",
    "#                 MultiHeadAttention(num_heads, key_dim=embedding_size),\n",
    "#                 LayerNormalization(),\n",
    "#                 Dense(forward_expansion * embedding_size, activation='relu'),\n",
    "#                 Dropout(dropout),\n",
    "#                 Dense(embedding_size),\n",
    "#                 LayerNormalization()\n",
    "#             ]) for _ in range(num_encoder_layers)\n",
    "#         ]\n",
    "\n",
    "#         self.decoder_layers = [\n",
    "#             tf.keras.Sequential([\n",
    "#                 MultiHeadAttention(num_heads, key_dim=embedding_size),\n",
    "#                 LayerNormalization(),\n",
    "#                 Dense(forward_expansion * embedding_size, activation='relu'),\n",
    "#                 Dropout(dropout),\n",
    "#                 Dense(embedding_size),\n",
    "#                 LayerNormalization()\n",
    "#             ]) for _ in range(num_decoder_layers)\n",
    "#         ]\n",
    "\n",
    "#         self.final_layer = Dense(trg_vocab_size)\n",
    "\n",
    "#     def call(self, src, trg):\n",
    "#         seq_len_src = tf.shape(src)[1]\n",
    "#         seq_len_trg = tf.shape(trg)[1]\n",
    "\n",
    "#         src_positions = tf.range(start=0, limit=seq_len_src, delta=1)\n",
    "#         trg_positions = tf.range(start=0, limit=seq_len_trg, delta=1)\n",
    "\n",
    "#         src_embeddings = self.src_embedding(src) + self.src_pos_embedding(src_positions)\n",
    "#         trg_embeddings = self.trg_embedding(trg) + self.trg_pos_embedding(trg_positions)\n",
    "\n",
    "#         for layer in self.encoder_layers:\n",
    "#             src_embeddings = layer(src_embeddings)\n",
    "\n",
    "#         for layer in self.decoder_layers:\n",
    "#             trg_embeddings = layer(trg_embeddings)\n",
    "\n",
    "#         outputs = self.final_layer(trg_embeddings)\n",
    "#         return outputs\n",
    "\n",
    "# # Hyperparameters\n",
    "# src_vocab_size = 256\n",
    "# trg_vocab_size = len(token_Sentence.word_index)\n",
    "# embedding_size = 512\n",
    "# num_heads = 8\n",
    "# num_encoder_layers = 3\n",
    "# num_decoder_layers = 3\n",
    "# dropout = 0.10\n",
    "# max_len_s = Images.shape[1]\n",
    "# max_len_t = len(Sentence[0])\n",
    "# forward_expansion = 4\n",
    "# learning_rate = 3e-4\n",
    "# batch_size = 1\n",
    "\n",
    "# model = TransformerModel(\n",
    "#     src_vocab_size, trg_vocab_size, embedding_size, num_heads,\n",
    "#     num_encoder_layers, num_decoder_layers, forward_expansion, dropout,\n",
    "#     max_len_s, max_len_t\n",
    "# )\n",
    "\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "# loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# # Training Function\n",
    "# def create_batch(src, tar, batchsize, i):\n",
    "#     src, tar = src[(i-1)*batchsize:(i-1)*batchsize + batchsize], tar[(i-1)*batchsize:(i-1)*batchsize + batchsize]\n",
    "#     return tf.convert_to_tensor(src, dtype=tf.int32), tf.convert_to_tensor(tar, dtype=tf.int32)\n",
    "\n",
    "# def train_step(src, tar):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         predictions = model(src, tar[:, :-1])\n",
    "#         loss = loss_object(tar[:, 1:], predictions)\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "#     return loss\n",
    "\n",
    "# # Training Loop\n",
    "# for epoch in range(1, 3):\n",
    "#     total_loss = 0\n",
    "#     for i in range(1, len(Images) // batch_size):\n",
    "#         src, tar = create_batch(Images, Sentence, batch_size, i)\n",
    "#         loss = train_step(src, tar)\n",
    "#         total_loss += loss\n",
    "\n",
    "#     print(f'| end of epoch {epoch} | Training loss {total_loss.numpy():.2f} |')\n",
    "\n",
    "# # Evaluation and Image Display\n",
    "# def display_image(name):\n",
    "#     img_path = os.path.join(data_dir, 'images', name)\n",
    "#     img = image.load_img(img_path, target_size=(512, 512, 3))\n",
    "#     img = image.img_to_array(img)\n",
    "#     img = img / 255\n",
    "#     plt.imshow(img)\n",
    "\n",
    "# def evaluate(index):\n",
    "#     img_path = train_captions.iloc[index]['image']\n",
    "#     caption = train_captions.iloc[index]['caption']\n",
    "#     img = load_image(img_path)\n",
    "#     img_arr = np.array([img])\n",
    "#     sentence = [f'<SOS> {caption} <EOS>']\n",
    "#     sentence = pad_sequences_fixed(token_Sentence.texts_to_sequences(sentence), maxlen=max_len_t)\n",
    "\n",
    "#     src = tf.convert_to_tensor(img_arr, dtype=tf.int32)\n",
    "#     tar = tf.convert_to_tensor(sentence, dtype=tf.int32)\n",
    "\n",
    "#     predictions = model(src, tar[:, :-1])\n",
    "#     sentence_formed = ''\n",
    "#     predicted_ids = tf.argmax(predictions, axis=-1).numpy()[0]\n",
    "#     for word_id in predicted_ids:\n",
    "#         if word_id == 3:  # EOS\n",
    "#             break\n",
    "#         for key, value in token_Sentence.word_index.items():\n",
    "#             if value == word_id and value != 2:  # SOS\n",
    "#                 sentence_formed += key + ' '\n",
    "#                 break\n",
    "\n",
    "#     display_image(img_path)\n",
    "#     return sentence_formed\n",
    "\n",
    "# # Example Evaluation\n",
    "# evaluate(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9d460-50b4-430b-80a9-80adc1c6899c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843999a0-e09c-48db-9de8-50f67fcc3748",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e01247-6ae0-44f0-ba4c-9af1e7b0af5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbb297e6-74a5-4917-a0e9-4403522deafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              image                                            caption\n",
      "0  000000203564.jpg  A bicycle replica with a clock as the front wh...\n",
      "1  000000322141.jpg  A room with blue walls and a white sink and door.\n",
      "2  000000016977.jpg  A car that seems to be parked illegally behind...\n",
      "3  000000106140.jpg  A large passenger airplane flying through the ...\n",
      "4  000000106140.jpg  There is a GOL plane taking off in a partly cl...\n",
      "\n",
      "              image                                            caption\n",
      "0  000000179765.jpg  A black Honda motorcycle parked in front of a ...\n",
      "1  000000179765.jpg      A Honda motorcycle parked in a grass driveway\n",
      "2  000000190236.jpg  An office cubicle with four different types of...\n",
      "3  000000331352.jpg          A small closed toilet in a cramped space.\n",
      "4  000000517069.jpg     Two women waiting at a bench next to a street.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_dir = r'C:\\Users\\UTKARSH\\Desktop\\visual intelligence'\n",
    "data_dir = r'C:\\Users\\UTKARSH\\Desktop\\visual intelligence\\data'\n",
    "models_dir = r'C:\\Users\\UTKARSH\\Desktop\\visual intelligence\\models'\n",
    "pre_processed_data_dir = r'C:\\Users\\UTKARSH\\Desktop\\visual intelligence\\pre_processed_data'\n",
    "\n",
    "# File path to the annotations CSV files\n",
    "train_annotations_file = os.path.join(pre_processed_data_dir, 'train_captions.csv')\n",
    "test_annotations_file = os.path.join(pre_processed_data_dir, 'test_captions.csv')\n",
    "\n",
    "# Check if the train CSV file exists\n",
    "if not os.path.exists(train_annotations_file):\n",
    "    raise FileNotFoundError(f\"File not found: {train_annotations_file}\")\n",
    "\n",
    "# Load the train captions CSV into DataFrame\n",
    "train_captions = pd.read_csv(train_annotations_file)\n",
    "print(train_captions.head())\n",
    "\n",
    "# Check if the test CSV file exists\n",
    "if not os.path.exists(test_annotations_file):\n",
    "    raise FileNotFoundError(f\"File not found: {test_annotations_file}\")\n",
    "\n",
    "# Load the test captions CSV into DataFrame\n",
    "test_captions = pd.read_csv(test_annotations_file)\n",
    "print()\n",
    "print(test_captions.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e8a5121-4b06-4711-ba38-0bbe4d83fa3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   image    100 non-null    object\n",
      " 1   caption  100 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "train_captions = train_captions.head(100)\n",
    "test_captions = test_captions.head(100)\n",
    "\n",
    "print(train_captions.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cbadc77-fdfa-475e-be85-a24d48d93804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=[<tf.Tensor: shape=(1, 3072, 512), dtype=float32, numpy=\n",
      "array([[[-0.04444816,  0.05680621,  0.03162703, ..., -0.02467461,\n",
      "          0.0298239 ,  0.02678167],\n",
      "        [ 0.01195009,  0.05360465, -0.05278448, ..., -0.02060539,\n",
      "          0.03348603, -0.06297432],\n",
      "        [-0.06714374, -0.01851696,  0.02427966, ..., -0.01594974,\n",
      "          0.04868934,  0.01564619],\n",
      "        ...,\n",
      "        [-0.07674284,  0.04282918,  0.007847  , ...,  0.05232377,\n",
      "          0.00994828, -0.05000573],\n",
      "        [ 0.01174462,  0.02788119, -0.06101137, ..., -0.00800679,\n",
      "          0.06449707, -0.02551356],\n",
      "        [ 0.01279427,  0.02897159, -0.06082598, ..., -0.01330704,\n",
      "          0.02632045, -0.06084739]]], dtype=float32)>, <tf.Tensor: shape=(1, 3072, 512), dtype=float32, numpy=\n",
      "array([[[-0.04444816,  0.05680621,  0.03162703, ..., -0.02467461,\n",
      "          0.0298239 ,  0.02678167],\n",
      "        [ 0.01195009,  0.05360465, -0.05278448, ..., -0.02060539,\n",
      "          0.03348603, -0.06297432],\n",
      "        [-0.06714374, -0.01851696,  0.02427966, ..., -0.01594974,\n",
      "          0.04868934,  0.01564619],\n",
      "        ...,\n",
      "        [-0.07674284,  0.04282918,  0.007847  , ...,  0.05232377,\n",
      "          0.00994828, -0.05000573],\n",
      "        [ 0.01174462,  0.02788119, -0.06101137, ..., -0.00800679,\n",
      "          0.06449707, -0.02551356],\n",
      "        [ 0.01279427,  0.02897159, -0.06082598, ..., -0.01330704,\n",
      "          0.02632045, -0.06084739]]], dtype=float32)>, <tf.Tensor: shape=(1, 3072, 512), dtype=float32, numpy=\n",
      "array([[[-0.04444816,  0.05680621,  0.03162703, ..., -0.02467461,\n",
      "          0.0298239 ,  0.02678167],\n",
      "        [ 0.01195009,  0.05360465, -0.05278448, ..., -0.02060539,\n",
      "          0.03348603, -0.06297432],\n",
      "        [-0.06714374, -0.01851696,  0.02427966, ..., -0.01594974,\n",
      "          0.04868934,  0.01564619],\n",
      "        ...,\n",
      "        [-0.07674284,  0.04282918,  0.007847  , ...,  0.05232377,\n",
      "          0.00994828, -0.05000573],\n",
      "        [ 0.01174462,  0.02788119, -0.06101137, ..., -0.00800679,\n",
      "          0.06449707, -0.02551356],\n",
      "        [ 0.01279427,  0.02897159, -0.06082598, ..., -0.01330704,\n",
      "          0.02632045, -0.06084739]]], dtype=float32)>]. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"sequential\" \"                 f\"(type Sequential).\n\nMultiHeadAttention.call() missing 1 required positional argument: 'value'\n\nCall arguments received by layer \"sequential\" \"                 f\"(type Sequential):\n  • inputs=['tf.Tensor(shape=(1, 3072, 512), dtype=float32)', 'tf.Tensor(shape=(1, 3072, 512), dtype=float32)', 'tf.Tensor(shape=(1, 3072, 512), dtype=float32)']\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 130\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(Images) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size):\n\u001b[0;32m    129\u001b[0m     src, tar \u001b[38;5;241m=\u001b[39m create_batch(Images, Sentence, batch_size, i)\n\u001b[1;32m--> 130\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m| end of epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Training loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m |\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 118\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(src, tar)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(src, tar):\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m--> 118\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtar\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_object(tar[:, \u001b[38;5;241m1\u001b[39m:], predictions)\n\u001b[0;32m    120\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[10], line 79\u001b[0m, in \u001b[0;36mTransformerModel.call\u001b[1;34m(self, src, trg)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Encoder: Self-attention on the input sequence\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers:\n\u001b[1;32m---> 79\u001b[0m     src_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msrc_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_embeddings\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# Decoder: Self-attention on the target sequence, with cross-attention to encoder output\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers:\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling layer \"sequential\" \"                 f\"(type Sequential).\n\nMultiHeadAttention.call() missing 1 required positional argument: 'value'\n\nCall arguments received by layer \"sequential\" \"                 f\"(type Sequential):\n  • inputs=['tf.Tensor(shape=(1, 3072, 512), dtype=float32)', 'tf.Tensor(shape=(1, 3072, 512), dtype=float32)', 'tf.Tensor(shape=(1, 3072, 512), dtype=float32)']\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "\n",
    "# Image Loading Function\n",
    "def load_image(name):\n",
    "    img_path = os.path.join(data_dir, 'train', name)  # Adjust path if needed\n",
    "    img = image.load_img(img_path, target_size=(32, 32, 3))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.reshape(img, (32 * 32 * 3))\n",
    "    return img\n",
    "\n",
    "# Process Images and Captions for Training Data\n",
    "image_arr, sentence_arr = [], []\n",
    "for index, row in train_captions.iterrows():\n",
    "    img_path = row['image']\n",
    "    caption = '<SOS> ' + row['caption'] + ' <EOS>'\n",
    "    image_arr.append(load_image(img_path))\n",
    "    sentence_arr.append(caption)\n",
    "\n",
    "Images = np.array(image_arr)\n",
    "\n",
    "# Tokenization and Padding\n",
    "def tokenize(texts):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return sequences, tokenizer\n",
    "\n",
    "def pad_sequences_fixed(sequences, maxlen=None):\n",
    "    return pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "\n",
    "Sentence, token_Sentence = tokenize(sentence_arr)\n",
    "Sentence = pad_sequences_fixed(Sentence)\n",
    "\n",
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, embedding_size, num_heads,\n",
    "                 num_encoder_layers, num_decoder_layers, forward_expansion, dropout,\n",
    "                 max_len_s, max_len_t):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        self.src_embedding = tf.keras.layers.Embedding(src_vocab_size, embedding_size)\n",
    "        self.trg_embedding = tf.keras.layers.Embedding(trg_vocab_size, embedding_size)\n",
    "        self.src_pos_embedding = tf.keras.layers.Embedding(max_len_s, embedding_size)\n",
    "        self.trg_pos_embedding = tf.keras.layers.Embedding(max_len_t, embedding_size)\n",
    "\n",
    "        self.encoder_layers = [\n",
    "            tf.keras.Sequential([\n",
    "                tf.keras.layers.MultiHeadAttention(num_heads, key_dim=embedding_size),\n",
    "                tf.keras.layers.LayerNormalization(),\n",
    "                tf.keras.layers.Dense(forward_expansion * embedding_size, activation='relu'),\n",
    "                tf.keras.layers.Dropout(dropout),\n",
    "                tf.keras.layers.Dense(embedding_size),\n",
    "                tf.keras.layers.LayerNormalization()\n",
    "            ]) for _ in range(num_encoder_layers)\n",
    "        ]\n",
    "\n",
    "        self.decoder_layers = [\n",
    "            tf.keras.Sequential([\n",
    "                tf.keras.layers.MultiHeadAttention(num_heads, key_dim=embedding_size),\n",
    "                tf.keras.layers.LayerNormalization(),\n",
    "                tf.keras.layers.Dense(forward_expansion * embedding_size, activation='relu'),\n",
    "                tf.keras.layers.Dropout(dropout),\n",
    "                tf.keras.layers.Dense(embedding_size),\n",
    "                tf.keras.layers.LayerNormalization()\n",
    "            ]) for _ in range(num_decoder_layers)\n",
    "        ]\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(trg_vocab_size)\n",
    "\n",
    "    def call(self, src, trg):\n",
    "        seq_len_src = tf.shape(src)[1]\n",
    "        seq_len_trg = tf.shape(trg)[1]\n",
    "\n",
    "        src_positions = tf.range(start=0, limit=seq_len_src, delta=1)\n",
    "        trg_positions = tf.range(start=0, limit=seq_len_trg, delta=1)\n",
    "\n",
    "        src_embeddings = self.src_embedding(src) + self.src_pos_embedding(src_positions)\n",
    "        trg_embeddings = self.trg_embedding(trg) + self.trg_pos_embedding(trg_positions)\n",
    "\n",
    "        # Encoder: Self-attention on the input sequence\n",
    "        for layer in self.encoder_layers:\n",
    "            src_embeddings = layer([src_embeddings, src_embeddings, src_embeddings])\n",
    "\n",
    "        # Decoder: Self-attention on the target sequence, with cross-attention to encoder output\n",
    "        for layer in self.decoder_layers:\n",
    "            trg_embeddings = layer([trg_embeddings, src_embeddings, src_embeddings])\n",
    "\n",
    "        outputs = self.final_layer(trg_embeddings)\n",
    "        return outputs\n",
    "\n",
    "# Hyperparameters\n",
    "src_vocab_size = 256\n",
    "trg_vocab_size = len(token_Sentence.word_index)\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dropout = 0.10\n",
    "max_len_s = Images.shape[1]\n",
    "max_len_t = len(Sentence[0])\n",
    "forward_expansion = 4\n",
    "learning_rate = 3e-4\n",
    "batch_size = 1\n",
    "\n",
    "model = TransformerModel(\n",
    "    src_vocab_size, trg_vocab_size, embedding_size, num_heads,\n",
    "    num_encoder_layers, num_decoder_layers, forward_expansion, dropout,\n",
    "    max_len_s, max_len_t\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Training Function\n",
    "def create_batch(src, tar, batchsize, i):\n",
    "    src, tar = src[(i-1)*batchsize:(i-1)*batchsize + batchsize], tar[(i-1)*batchsize:(i-1)*batchsize + batchsize]\n",
    "    return tf.convert_to_tensor(src, dtype=tf.int32), tf.convert_to_tensor(tar, dtype=tf.int32)\n",
    "\n",
    "def train_step(src, tar):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(src, tar[:, :-1])\n",
    "        loss = loss_object(tar[:, 1:], predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(1, 3):\n",
    "    total_loss = 0\n",
    "    for i in range(1, len(Images) // batch_size):\n",
    "        src, tar = create_batch(Images, Sentence, batch_size, i)\n",
    "        loss = train_step(src, tar)\n",
    "        total_loss += loss\n",
    "\n",
    "    print(f'| end of epoch {epoch} | Training loss {total_loss.numpy():.2f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a00b4-a88f-4f15-9dcd-e0b62151bfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # Evaluation and Image Display\n",
    "# def display_image(name):\n",
    "#     img_path = os.path.join(data_dir, 'images', name)\n",
    "#     img = image.load_img(img_path, target_size=(512, 512, 3))\n",
    "#     img = image.img_to_array(img)\n",
    "#     img = img / 255\n",
    "#     plt.imshow(img)\n",
    "\n",
    "# def evaluate(index):\n",
    "#     img_path = train_captions.iloc[index]['image']\n",
    "#     caption = train_captions.iloc[index]['caption']\n",
    "#     img = load_image(img_path)\n",
    "#     img_arr = np.array([img])\n",
    "#     sentence = [f'<SOS> {caption} <EOS>']\n",
    "#     sentence = pad_sequences_fixed(token_Sentence.texts_to_sequences(sentence), maxlen=max_len_t)\n",
    "\n",
    "#     src = tf.convert_to_tensor(img_arr, dtype=tf.int32)\n",
    "#     tar = tf.convert_to_tensor(sentence, dtype=tf.int32)\n",
    "\n",
    "#     predictions = model(src, tar[:, :-1])\n",
    "#     sentence_formed = ''\n",
    "#     predicted_ids = tf.argmax(predictions, axis=-1).numpy()[0]\n",
    "#     for word_id in predicted_ids:\n",
    "#         if word_id == 3:  # EOS\n",
    "#             break\n",
    "#         for key, value in token_Sentence.word_index.items():\n",
    "#             if value == word_id and value != 2:  # SOS\n",
    "#                 sentence_formed += key + ' '\n",
    "#                 break\n",
    "\n",
    "#     display_image(img_path)\n",
    "#     return sentence_formed\n",
    "\n",
    "# # Example Evaluation\n",
    "# evaluate(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de140bc-fa0b-4e92-a210-5bccb8080c50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97fe7c8-3e8c-4099-971b-0373de6ce0f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e97b623-604f-423a-9d0b-88835bb1dee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              image                                            caption\n",
      "0  000000203564.jpg  A bicycle replica with a clock as the front wh...\n",
      "1  000000322141.jpg  A room with blue walls and a white sink and door.\n",
      "2  000000016977.jpg  A car that seems to be parked illegally behind...\n",
      "3  000000106140.jpg  A large passenger airplane flying through the ...\n",
      "4  000000106140.jpg  There is a GOL plane taking off in a partly cl...\n",
      "\n",
      "              image                                            caption\n",
      "0  000000179765.jpg  A black Honda motorcycle parked in front of a ...\n",
      "1  000000179765.jpg      A Honda motorcycle parked in a grass driveway\n",
      "2  000000190236.jpg  An office cubicle with four different types of...\n",
      "3  000000331352.jpg          A small closed toilet in a cramped space.\n",
      "4  000000517069.jpg     Two women waiting at a bench next to a street.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   image    10 non-null     object\n",
      " 1   caption  10 non-null     object\n",
      "dtypes: object(2)\n",
      "memory usage: 288.0+ bytes\n",
      "None\n",
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=[<tf.Tensor: shape=(1, 224, 224, 512), dtype=float32, numpy=\n",
      "array([[[[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]],\n",
      "\n",
      "        [[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]],\n",
      "\n",
      "        [[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]],\n",
      "\n",
      "        [[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]],\n",
      "\n",
      "        [[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]]]], dtype=float32)>, <tf.Tensor: shape=(1, 224, 224, 512), dtype=float32, numpy=\n",
      "array([[[[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]],\n",
      "\n",
      "        [[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]],\n",
      "\n",
      "        [[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]],\n",
      "\n",
      "        [[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]],\n",
      "\n",
      "        [[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]]]], dtype=float32)>, <tf.Tensor: shape=(1, 224, 224, 512), dtype=float32, numpy=\n",
      "array([[[[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]],\n",
      "\n",
      "        [[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]],\n",
      "\n",
      "        [[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]],\n",
      "\n",
      "        [[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]],\n",
      "\n",
      "        [[ 0.03972466, -0.14208332,  0.09750789, ...,  0.16138257,\n",
      "           0.12149117,  0.08362962],\n",
      "         [ 0.09185086, -0.15813312,  0.0942454 , ...,  0.11247592,\n",
      "           0.1532595 ,  0.02306019],\n",
      "         [ 0.03404172, -0.09741795,  0.13227373, ...,  0.14901204,\n",
      "           0.08486571,  0.04117691],\n",
      "         ...,\n",
      "         [ 0.01011723, -0.136559  ,  0.04208552, ...,  0.08774933,\n",
      "           0.08624677, -0.00203792],\n",
      "         [ 0.01120982, -0.10334083,  0.08202849, ...,  0.08906636,\n",
      "           0.10289542,  0.08148998],\n",
      "         [ 0.05488301, -0.0819671 ,  0.10505681, ...,  0.08244227,\n",
      "           0.1604034 ,  0.06969406]]]], dtype=float32)>]. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Exception encountered when calling layer \"sequential_24\" \"                 f\"(type Sequential).\n\nMultiHeadAttention.call() missing 1 required positional argument: 'value'\n\nCall arguments received by layer \"sequential_24\" \"                 f\"(type Sequential):\n  • inputs=['tf.Tensor(shape=(1, 224, 224, 512), dtype=float32)', 'tf.Tensor(shape=(1, 224, 224, 512), dtype=float32)', 'tf.Tensor(shape=(1, 224, 224, 512), dtype=float32)']\n  • training=None\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 191\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(Images) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m batch_size):\n\u001b[0;32m    190\u001b[0m     src, tar \u001b[38;5;241m=\u001b[39m create_batch(Images, Sentence, batch_size, i)\n\u001b[1;32m--> 191\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m| end of epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Training loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m |\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[14], line 180\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(src, tar)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(src, tar):\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[1;32m--> 180\u001b[0m         predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtar\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m         loss \u001b[38;5;241m=\u001b[39m loss_object(tar[:, \u001b[38;5;241m1\u001b[39m:], predictions)\n\u001b[0;32m    182\u001b[0m     gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, model\u001b[38;5;241m.\u001b[39mtrainable_variables)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\py310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[14], line 132\u001b[0m, in \u001b[0;36mTransformerModel.call\u001b[1;34m(self, src, trg)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Encoder: Self-attention on the input sequence\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder_layers:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# Self-attention: The query, key, and value are all the same (src_embeddings)\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m     src_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msrc_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_embeddings\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Decoder: Self-attention on the target sequence, with cross-attention to encoder output\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder_layers:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# Cross-attention: The query is the target embeddings (trg_embeddings)\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;66;03m# The key and value are the source embeddings (src_embeddings)\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: Exception encountered when calling layer \"sequential_24\" \"                 f\"(type Sequential).\n\nMultiHeadAttention.call() missing 1 required positional argument: 'value'\n\nCall arguments received by layer \"sequential_24\" \"                 f\"(type Sequential):\n  • inputs=['tf.Tensor(shape=(1, 224, 224, 512), dtype=float32)', 'tf.Tensor(shape=(1, 224, 224, 512), dtype=float32)', 'tf.Tensor(shape=(1, 224, 224, 512), dtype=float32)']\n  • training=None\n  • mask=None"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_dir = r'C:\\Users\\UTKARSH\\Desktop\\visual intelligence'\n",
    "data_dir = r'C:\\Users\\UTKARSH\\Desktop\\visual intelligence\\data'\n",
    "models_dir = r'C:\\Users\\UTKARSH\\Desktop\\visual intelligence\\models'\n",
    "pre_processed_data_dir = r'C:\\Users\\UTKARSH\\Desktop\\visual intelligence\\pre_processed_data'\n",
    "\n",
    "# File path to the annotations CSV files\n",
    "train_annotations_file = os.path.join(pre_processed_data_dir, 'train_captions.csv')\n",
    "test_annotations_file = os.path.join(pre_processed_data_dir, 'test_captions.csv')\n",
    "\n",
    "# Check if the train CSV file exists\n",
    "if not os.path.exists(train_annotations_file):\n",
    "    raise FileNotFoundError(f\"File not found: {train_annotations_file}\")\n",
    "\n",
    "# Load the train captions CSV into DataFrame\n",
    "train_captions = pd.read_csv(train_annotations_file)\n",
    "print(train_captions.head())\n",
    "\n",
    "# Check if the test CSV file exists\n",
    "if not os.path.exists(test_annotations_file):\n",
    "    raise FileNotFoundError(f\"File not found: {test_annotations_file}\")\n",
    "\n",
    "# Load the test captions CSV into DataFrame\n",
    "test_captions = pd.read_csv(test_annotations_file)\n",
    "print()\n",
    "print(test_captions.head())\n",
    "\n",
    "train_captions = train_captions.head(10)\n",
    "test_captions = test_captions.head(10)\n",
    "\n",
    "print(train_captions.info())\n",
    "\n",
    "# Image Loading Function\n",
    "def load_image(name, target_size=(224, 224)):\n",
    "    img_path = os.path.join(data_dir, 'train', name)  # Adjust path if needed\n",
    "    img = image.load_img(img_path, target_size=target_size)\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    img = img / 255.0  # Normalize\n",
    "    return img\n",
    "\n",
    "# Process Images and Captions for Training Data\n",
    "image_arr, sentence_arr = [], []\n",
    "for index, row in train_captions.iterrows():\n",
    "    img_path = row['image']\n",
    "    caption = '<SOS> ' + row['caption'] + ' <EOS>'\n",
    "    image_arr.append(load_image(img_path))\n",
    "    sentence_arr.append(caption)\n",
    "\n",
    "Images = np.vstack(image_arr)\n",
    "\n",
    "# Tokenization and Padding\n",
    "def tokenize(texts):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return sequences, tokenizer\n",
    "\n",
    "def pad_sequences_fixed(sequences, maxlen=None):\n",
    "    return pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "\n",
    "Sentence, token_Sentence = tokenize(sentence_arr)\n",
    "Sentence = pad_sequences_fixed(Sentence)\n",
    "\n",
    "# Transformer Model Class\n",
    "class TransformerModel(tf.keras.Model):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, embedding_size, num_heads,\n",
    "                 num_encoder_layers, num_decoder_layers, forward_expansion, dropout,\n",
    "                 max_len_s, max_len_t):\n",
    "        super(TransformerModel, self).__init__()\n",
    "\n",
    "        # Image feature processing (flattened and passed through embedding)\n",
    "        self.src_embedding = tf.keras.layers.Dense(embedding_size)\n",
    "\n",
    "        # Caption token embedding\n",
    "        self.trg_embedding = tf.keras.layers.Embedding(trg_vocab_size, embedding_size)\n",
    "\n",
    "        # Positional embeddings for source and target\n",
    "        self.src_pos_embedding = tf.keras.layers.Embedding(max_len_s, embedding_size)\n",
    "        self.trg_pos_embedding = tf.keras.layers.Embedding(max_len_t, embedding_size)\n",
    "\n",
    "        # Encoder layers (self-attention)\n",
    "        self.encoder_layers = [\n",
    "            tf.keras.Sequential([\n",
    "                tf.keras.layers.MultiHeadAttention(num_heads, key_dim=embedding_size),\n",
    "                tf.keras.layers.LayerNormalization(),\n",
    "                tf.keras.layers.Dense(forward_expansion * embedding_size, activation='relu'),\n",
    "                tf.keras.layers.Dropout(dropout),\n",
    "                tf.keras.layers.Dense(embedding_size),\n",
    "                tf.keras.layers.LayerNormalization()\n",
    "            ]) for _ in range(num_encoder_layers)\n",
    "        ]\n",
    "\n",
    "        # Decoder layers (cross-attention and self-attention)\n",
    "        self.decoder_layers = [\n",
    "            tf.keras.Sequential([\n",
    "                tf.keras.layers.MultiHeadAttention(num_heads, key_dim=embedding_size),\n",
    "                tf.keras.layers.LayerNormalization(),\n",
    "                tf.keras.layers.Dense(forward_expansion * embedding_size, activation='relu'),\n",
    "                tf.keras.layers.Dropout(dropout),\n",
    "                tf.keras.layers.Dense(embedding_size),\n",
    "                tf.keras.layers.LayerNormalization()\n",
    "            ]) for _ in range(num_decoder_layers)\n",
    "        ]\n",
    "\n",
    "        # Output layer\n",
    "        self.final_layer = tf.keras.layers.Dense(trg_vocab_size)\n",
    "\n",
    "    def call(self, src, trg):\n",
    "        seq_len_src = tf.shape(src)[1]\n",
    "        seq_len_trg = tf.shape(trg)[1]\n",
    "\n",
    "        # Positional embeddings\n",
    "        src_positions = tf.range(start=0, limit=seq_len_src, delta=1)\n",
    "        trg_positions = tf.range(start=0, limit=seq_len_trg, delta=1)\n",
    "\n",
    "        # Embed the source and target sequences\n",
    "        src_embeddings = self.src_embedding(src) + self.src_pos_embedding(src_positions)\n",
    "        trg_embeddings = self.trg_embedding(trg) + self.trg_pos_embedding(trg_positions)\n",
    "\n",
    "        # Encoder: Self-attention on the input sequence\n",
    "        for layer in self.encoder_layers:\n",
    "            # Self-attention: The query, key, and value are all the same (src_embeddings)\n",
    "            src_embeddings = layer([src_embeddings, src_embeddings, src_embeddings])\n",
    "\n",
    "        # Decoder: Self-attention on the target sequence, with cross-attention to encoder output\n",
    "        for layer in self.decoder_layers:\n",
    "            # Cross-attention: The query is the target embeddings (trg_embeddings)\n",
    "            # The key and value are the source embeddings (src_embeddings)\n",
    "            trg_embeddings = layer([trg_embeddings, src_embeddings, src_embeddings])\n",
    "\n",
    "        # Final output layer\n",
    "        outputs = self.final_layer(trg_embeddings)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_size = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dropout = 0.10\n",
    "max_len_s = Images.shape[1]  # Image flattened dimension (e.g., 224x224x3 = 150528)\n",
    "max_len_t = len(Sentence[0])  # Max length of caption\n",
    "forward_expansion = 4\n",
    "learning_rate = 3e-4\n",
    "batch_size = 1\n",
    "\n",
    "# Vocabulary size for target (captions)\n",
    "src_vocab_size = 256  # Image features (fixed size after embedding)\n",
    "trg_vocab_size = len(token_Sentence.word_index) + 1  # Adding 1 for padding token\n",
    "\n",
    "# Initialize the model\n",
    "model = TransformerModel(\n",
    "    src_vocab_size, trg_vocab_size, embedding_size, num_heads,\n",
    "    num_encoder_layers, num_decoder_layers, forward_expansion, dropout,\n",
    "    max_len_s, max_len_t\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Training Function\n",
    "def create_batch(src, tar, batchsize, i):\n",
    "    src_batch = src[(i-1)*batchsize:(i-1)*batchsize + batchsize]\n",
    "    tar_batch = tar[(i-1)*batchsize:(i-1)*batchsize + batchsize]\n",
    "    return tf.convert_to_tensor(src_batch, dtype=tf.float32), tf.convert_to_tensor(tar_batch, dtype=tf.int32)\n",
    "\n",
    "def train_step(src, tar):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(src, tar[:, :-1])\n",
    "        loss = loss_object(tar[:, 1:], predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(1, 3):\n",
    "    total_loss = 0\n",
    "    for i in range(1, len(Images) // batch_size):\n",
    "        src, tar = create_batch(Images, Sentence, batch_size, i)\n",
    "        loss = train_step(src, tar)\n",
    "        total_loss += loss\n",
    "\n",
    "    print(f'| end of epoch {epoch} | Training loss {total_loss.numpy():.2f} |')\n",
    "+++++++++++++++++++++++++++++++++++\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8e43e8-76c7-4bed-89df-aca2182670a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
